{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1_19k508.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshithagopagani/NLP/blob/main/Assignment-1_19k508.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GAXsNlTVzel3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6043d3b8-15cd-48e4-b565-044c5c1209fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.21.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.0+cu113)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "MmNsIZl28LAs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = 'Text Simplification is the task of reducing the complexity of the vocabulary and sentence structure of text while retaining its original meaning, with the goal of improving readability and understanding.'\n",
        "documentB = 'Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. A sentiment analysis system for text analysis combines natural language processing (NLP) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or phrase.'"
      ],
      "metadata": {
        "id": "RvtRW5tEz-Y3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')"
      ],
      "metadata": {
        "id": "g4Nz-IU50P1K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
      ],
      "metadata": {
        "id": "nWROh6Rl0VAi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "    numOfWordsA[word] += 1\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "    numOfWordsB[word] += 1"
      ],
      "metadata": {
        "id": "iYgFBct80Yec"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y464OoO0daS",
        "outputId": "8102cafc-a2c5-430f-edf5-4d34e0a9197f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency (TF) \n",
        "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
      ],
      "metadata": {
        "id": "Rtmoly0d1StP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict"
      ],
      "metadata": {
        "id": "zQHLiXIj2Es4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
        "print(tfA)\n",
        "print(tfB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvduQkZi2WTY",
        "outputId": "17666fbe-0670-4242-dfdc-1b6eac4461af"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entities,': 0.0, 'themes': 0.0, 'within': 0.0, 'to': 0.0, 'language': 0.0, 'weighted': 0.0, 'Analysis': 0.0, 'scores': 0.0, 'writing': 0.0, 'topics,': 0.0, 'Sentiment': 0.0, 'determining': 0.0, 'phrase.': 0.0, 'text': 0.03333333333333333, 'original': 0.03333333333333333, 'of': 0.13333333333333333, 'sentence': 0.03333333333333333, 'readability': 0.03333333333333333, 'a': 0.0, 'goal': 0.03333333333333333, 'neutral.': 0.0, 'assign': 0.0, 'complexity': 0.03333333333333333, 'positive,': 0.0, 'process': 0.0, 'or': 0.0, 'processing': 0.0, 'whether': 0.0, '(NLP)': 0.0, 'combines': 0.0, 'while': 0.03333333333333333, 'categories': 0.0, 'natural': 0.0, 'meaning,': 0.03333333333333333, 'machine': 0.0, 'system': 0.0, 'and': 0.06666666666666667, 'reducing': 0.03333333333333333, 'vocabulary': 0.03333333333333333, 'improving': 0.03333333333333333, 'techniques': 0.0, 'learning': 0.0, 'Simplification': 0.03333333333333333, 'negative': 0.0, 'its': 0.03333333333333333, 'analysis': 0.0, 'understanding.': 0.03333333333333333, 'piece': 0.0, 'is': 0.03333333333333333, 'task': 0.03333333333333333, 'for': 0.0, 'the': 0.13333333333333333, 'sentiment': 0.0, 'A': 0.0, 'with': 0.03333333333333333, 'Text': 0.03333333333333333, 'structure': 0.03333333333333333, 'retaining': 0.03333333333333333}\n",
            "{'entities,': 0.02, 'themes': 0.02, 'within': 0.02, 'to': 0.04, 'language': 0.02, 'weighted': 0.02, 'Analysis': 0.02, 'scores': 0.02, 'writing': 0.02, 'topics,': 0.02, 'Sentiment': 0.02, 'determining': 0.02, 'phrase.': 0.02, 'text': 0.02, 'original': 0.0, 'of': 0.04, 'sentence': 0.02, 'readability': 0.0, 'a': 0.04, 'goal': 0.0, 'neutral.': 0.02, 'assign': 0.02, 'complexity': 0.0, 'positive,': 0.02, 'process': 0.02, 'or': 0.04, 'processing': 0.02, 'whether': 0.02, '(NLP)': 0.02, 'combines': 0.02, 'while': 0.0, 'categories': 0.02, 'natural': 0.02, 'meaning,': 0.0, 'machine': 0.02, 'system': 0.02, 'and': 0.04, 'reducing': 0.0, 'vocabulary': 0.0, 'improving': 0.0, 'techniques': 0.02, 'learning': 0.02, 'Simplification': 0.0, 'negative': 0.02, 'its': 0.0, 'analysis': 0.04, 'understanding.': 0.0, 'piece': 0.02, 'is': 0.04, 'task': 0.0, 'for': 0.02, 'the': 0.04, 'sentiment': 0.04, 'A': 0.02, 'with': 0.0, 'Text': 0.0, 'structure': 0.0, 'retaining': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inverse Data Frequency (IDF)**\n",
        "The log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus. "
      ],
      "metadata": {
        "id": "y0xnLIuQ2-Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict"
      ],
      "metadata": {
        "id": "MCHvwv4-3Cem"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "print(idfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz9l1xlw20pU",
        "outputId": "29024d1a-12ac-49d0-ce1e-0ad0306c02b8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entities,': 0.6931471805599453, 'themes': 0.6931471805599453, 'within': 0.6931471805599453, 'to': 0.6931471805599453, 'language': 0.6931471805599453, 'weighted': 0.6931471805599453, 'Analysis': 0.6931471805599453, 'scores': 0.6931471805599453, 'writing': 0.6931471805599453, 'topics,': 0.6931471805599453, 'Sentiment': 0.6931471805599453, 'determining': 0.6931471805599453, 'phrase.': 0.6931471805599453, 'text': 0.0, 'original': 0.6931471805599453, 'of': 0.0, 'sentence': 0.0, 'readability': 0.6931471805599453, 'a': 0.6931471805599453, 'goal': 0.6931471805599453, 'neutral.': 0.6931471805599453, 'assign': 0.6931471805599453, 'complexity': 0.6931471805599453, 'positive,': 0.6931471805599453, 'process': 0.6931471805599453, 'or': 0.6931471805599453, 'processing': 0.6931471805599453, 'whether': 0.6931471805599453, '(NLP)': 0.6931471805599453, 'combines': 0.6931471805599453, 'while': 0.6931471805599453, 'categories': 0.6931471805599453, 'natural': 0.6931471805599453, 'meaning,': 0.6931471805599453, 'machine': 0.6931471805599453, 'system': 0.6931471805599453, 'and': 0.0, 'reducing': 0.6931471805599453, 'vocabulary': 0.6931471805599453, 'improving': 0.6931471805599453, 'techniques': 0.6931471805599453, 'learning': 0.6931471805599453, 'Simplification': 0.6931471805599453, 'negative': 0.6931471805599453, 'its': 0.6931471805599453, 'analysis': 0.6931471805599453, 'understanding.': 0.6931471805599453, 'piece': 0.6931471805599453, 'is': 0.0, 'task': 0.6931471805599453, 'for': 0.6931471805599453, 'the': 0.0, 'sentiment': 0.6931471805599453, 'A': 0.6931471805599453, 'with': 0.6931471805599453, 'Text': 0.6931471805599453, 'structure': 0.6931471805599453, 'retaining': 0.6931471805599453}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lastly, the TF-IDF is simply the TF multiplied by IDF.**"
      ],
      "metadata": {
        "id": "iqfDkEwN3vMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "metadata": {
        "id": "qlinmoWg3yMF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgmGrl8d36n0",
        "outputId": "769ad0d5-095f-4755-f0a8-39ff2949389b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   entities,    themes    within        to  language  weighted  Analysis  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1   0.013863  0.013863  0.013863  0.027726  0.013863  0.013863  0.013863   \n",
            "\n",
            "     scores   writing   topics,  ...   is      task       for  the  sentiment  \\\n",
            "0  0.000000  0.000000  0.000000  ...  0.0  0.023105  0.000000  0.0   0.000000   \n",
            "1  0.013863  0.013863  0.013863  ...  0.0  0.000000  0.013863  0.0   0.027726   \n",
            "\n",
            "          A      with      Text  structure  retaining  \n",
            "0  0.000000  0.023105  0.023105   0.023105   0.023105  \n",
            "1  0.013863  0.000000  0.000000   0.000000   0.000000  \n",
            "\n",
            "[2 rows x 58 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMLWw1u34Efw",
        "outputId": "bd35417e-8254-47c9-ee2e-74e751ade759"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   analysis       and    assign  categories  combines  complexity  \\\n",
            "0  0.000000  0.233118  0.000000    0.000000  0.000000    0.163819   \n",
            "1  0.380656  0.180560  0.126885    0.126885  0.126885    0.000000   \n",
            "\n",
            "   determining  entities       for      goal  ...       to    topics  \\\n",
            "0     0.000000  0.000000  0.000000  0.163819  ...  0.00000  0.000000   \n",
            "1     0.126885  0.126885  0.126885  0.000000  ...  0.25377  0.126885   \n",
            "\n",
            "   understanding  vocabulary  weighted   whether     while      with  \\\n",
            "0       0.163819    0.163819  0.000000  0.000000  0.163819  0.163819   \n",
            "1       0.000000    0.000000  0.126885  0.126885  0.000000  0.000000   \n",
            "\n",
            "     within   writing  \n",
            "0  0.000000  0.000000  \n",
            "1  0.126885  0.126885  \n",
            "\n",
            "[2 rows x 53 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_model = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
        "model = SentenceTransformer(_model)"
      ],
      "metadata": {
        "id": "uLSCXrB78XQK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isSimilar(a,b):\n",
        "  threshold = 0.9\n",
        "  embeddings = model.encode([a, b])\n",
        "  embeddings.shape\n",
        "  res = list(cosine_similarity([embeddings[0]], embeddings[1:]))\n",
        "  return res[0]\n",
        "print(isSimilar(documentA, documentB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOPh4GyD9YcV",
        "outputId": "d6035f01-7bf8-4430-f28f-97e9f3994b30"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6463746]\n"
          ]
        }
      ]
    }
  ]
}